{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flowers_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MfuC83LYJl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "261729e8-6a74-4242-8599-1127f96ad2be"
      },
      "source": [
        "# Author: Matthew McFee\n",
        "# DAT112 - Assignment #1\n",
        "# July 7, 2020\n",
        "\n",
        "# Install wget with pip and download the data files\n",
        "\n",
        "# These should be commented in the actual .py as these are Jupyter Notebook\n",
        "# specific \n",
        "# !pip install wget\n",
        "\n",
        "# !wget \"https://support.scinet.utoronto.ca/~ejspence/50x50flowers.images.npy\"\n",
        "# !wget \"https://support.scinet.utoronto.ca/~ejspence/50x50flowers.targets.npy\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=160bf8859a21f23400a21750cc3b5cbed5dfb25e48bdfb456356f91bd33300a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "--2020-07-07 05:11:13--  https://support.scinet.utoronto.ca/~ejspence/50x50flowers.images.npy\n",
            "Resolving support.scinet.utoronto.ca (support.scinet.utoronto.ca)... 142.150.255.98\n",
            "Connecting to support.scinet.utoronto.ca (support.scinet.utoronto.ca)|142.150.255.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81600128 (78M) [text/plain]\n",
            "Saving to: ‘50x50flowers.images.npy’\n",
            "\n",
            "50x50flowers.images 100%[===================>]  77.82M  42.6MB/s    in 1.8s    \n",
            "\n",
            "2020-07-07 05:11:15 (42.6 MB/s) - ‘50x50flowers.images.npy’ saved [81600128/81600128]\n",
            "\n",
            "--2020-07-07 05:11:16--  https://support.scinet.utoronto.ca/~ejspence/50x50flowers.targets.npy\n",
            "Resolving support.scinet.utoronto.ca (support.scinet.utoronto.ca)... 142.150.255.98\n",
            "Connecting to support.scinet.utoronto.ca (support.scinet.utoronto.ca)|142.150.255.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11008 (11K) [text/plain]\n",
            "Saving to: ‘50x50flowers.targets.npy’\n",
            "\n",
            "50x50flowers.target 100%[===================>]  10.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-07 05:11:16 (209 MB/s) - ‘50x50flowers.targets.npy’ saved [11008/11008]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEfwvpj1Y3G8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ac348728-0d61-48d9-ce54-d5d7edf9cc1f"
      },
      "source": [
        "# Load the data files\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_data(image_path, targets_path):\n",
        "\n",
        "  print(\"Reading flowers input file.\")\n",
        "  images = np.load(image_path)\n",
        "\n",
        "  print(\"Reading flowers target file.\")\n",
        "  targets = np.load(targets_path)\n",
        "\n",
        "  return images, targets\n",
        "\n",
        "# Hardcoded file paths because the assignment guidelines said this is okay\n",
        "IMAGE_PATH = \"50x50flowers.images.npy\"\n",
        "TARGETS_PATH = \"50x50flowers.targets.npy\"\n",
        "\n",
        "images, targets = load_data(IMAGE_PATH, TARGETS_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading flowers input file.\n",
            "Reading flowers target file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt2iEyrjZOx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13100bba-ac76-47b5-f02f-0a8aea9c7534"
      },
      "source": [
        "# Generate training, and testing data sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setting 30% of the data to be used as the test set\n",
        "# Setting random state for reproducibility \n",
        "X_train, X_test, y_train, y_test = train_test_split(images, targets,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=1)\n",
        "\n",
        "# Preprocess data \n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Odd issue where to_categorical adds an additional 18th class, solution is to \n",
        "# subtract 1 from all labels, so class labels start from 0\n",
        "\n",
        "y_train = y_train - 1\n",
        "y_test = y_test - 1\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=17)\n",
        "y_test = to_categorical(y_test, num_classes=17)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b1KgXUCaw-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bab47006-55b6-44bf-90e1-e2eb376b43af"
      },
      "source": [
        "# Function that builds a model \n",
        "\n",
        "# The model will be a simple convolutional neural network\n",
        "\n",
        "# Import the necessary Keras libaries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "\n",
        "# This is a simple model for part 1 to show that we have overfitting problems\n",
        "def build_simple_model():\n",
        "  \n",
        "  print(\"Building network.\")\n",
        "\n",
        "  # Using a sequential model\n",
        "  model = Sequential()\n",
        "  \n",
        "  # First convolutional layer, with input shape (50, 50, 3)\n",
        "  model.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(50,50,3)))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=1, activation=\"relu\"))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "  \n",
        "  # Flatten the data for final categorization\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Final classification layer, using softmax, with 17 nodes since there are \n",
        "  # 17 classes \n",
        "  model.add(Dense(17, activation=\"softmax\"))\n",
        "\n",
        "  return model\n",
        "\n",
        "# This is my complex model for part 2\n",
        "def build_complex_model():\n",
        "  \n",
        "  print(\"Building network.\")\n",
        "\n",
        "  # Using a sequential model\n",
        "  model = Sequential()\n",
        "  \n",
        "  # First convolutional layer, with input shape (50, 50, 3)\n",
        "  model.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(50,50,3)))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Additional convolutional layers. I played around with the number and number\n",
        "  # of filters per convolution layer. I've seen a trend of using 32 and then 64, \n",
        "  # and then 128 etc. \n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "  # Flatten the data for input into a densely connect network\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Used fully connected dense layers with dropout regularization. I played \n",
        "  # around with the dropout and found 0.5 seems to regulate overfitting better\n",
        "  # than other values. Strong regularization was needed as the model was \n",
        "  # overfitting badly without it.\n",
        "  model.add(Dense(64, activatidoes model train on validation datadoes model train on validation dataon=\"relu\"))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Final classification layer, using softmax, with 17 nodes since there are \n",
        "  # 17 classes \n",
        "  model.add(Dense(17, activation=\"softmax\"))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVaN7MdvLSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_complex_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRPPghCkc0YM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These function will actually train the model on the training set, and then run\n",
        "# the test data set through the trained model, and report the metrics specified \n",
        "# in the asssignment criteria\n",
        "\n",
        "def compile_model(model, optimizer_func, loss_func, metrics):\n",
        "\n",
        "  # Compile the model using the select optimizer function, loss function,\n",
        "  # and appropriate metrics \n",
        "\n",
        "  model.compile(optimizer=optimizer_func, loss=loss_func, \n",
        "                metrics=metrics)\n",
        "  return model\n",
        "\n",
        "def train_and_test_model(model, X_train, X_test, y_train, y_test, epochs,\n",
        "                         batch_size):\n",
        "\n",
        "  print(\"Training network.\")\n",
        "\n",
        "  # We store the return values of model.fit in the history variable, for easy\n",
        "  # plotting of loss  function during model training\n",
        "  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "                      epochs=epochs, batch_size=batch_size)\n",
        "  \n",
        "  return history \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coI-6NPIe7NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This was for the first model I tried, and I used it as a performance baseline\n",
        "\n",
        "# Build, compile, train, and test the model\n",
        "\n",
        "# model = build_simple_model()\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# These can be adjusted as needed \n",
        "# OPTIMIZER = \"adam\"\n",
        "# LOSS_FUNC = \"categorical_crossentropy\"\n",
        "# METRICS = [\"accuracy\"]\n",
        "\n",
        "# model = compile_model(model, OPTIMIZER, LOSS_FUNC, METRICS)\n",
        "\n",
        "# history = train_and_test_model(model, X_train, X_test, y_train, y_test, 10, 100)\n",
        "\n",
        "# Visualize loss function as a function of epoch for model training\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.title(\"Loss vs. Epoch (Model Training)\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A09IGOn_gyXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the final test set accuracy\n",
        "\n",
        "# print(\"The training score is \", history.history[\"loss\"][-1], \n",
        "      # history.history[\"accuracy\"][-1])\n",
        "\n",
        "# print(\"The test score is\", history.history[\"val_loss\"][-1], \n",
        "      # history.history[\"val_accuracy\"][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9SFkeD6kDkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It is clear that we are overfitting on the training set with even \n",
        "# a simple one layer convolutional network\n",
        "\n",
        "# The purpose of this code will be to generate new training data by\n",
        "# modifying existing data points in meaningful ways. Luckily, this \n",
        "# is already easy to do using Keras.\n",
        "\n",
        "# Trying the stock Keras docs example configuration with added rescaling\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    # Important to rescale data for neural networks\n",
        "    rescale=1./255,\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "valgen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Use the fit method on the existing training data\n",
        "\n",
        "# Train the data generator classes on the available images\n",
        "datagen.fit(X_train)\n",
        "valgen.fit(X_test)\n",
        "\n",
        "# Adapt the old train and test function for use with data generators\n",
        "def train_test_model_with_generator(model, datagen, X_train, y_train,\n",
        "                                    X_test, y_test, epochs, batch_size):\n",
        "\n",
        "  print(\"Training network.\")\n",
        "  \n",
        "  # We store the return values of model.fit in the history variable, for easy\n",
        "  # plotting of loss  function during model training\n",
        "  #history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), \n",
        "  #                    steps_per_epoch=len(X_train)/32, \n",
        "  #                    validation_data=(X_test, y_test), \n",
        "  #                    epochs=epochs)\n",
        "\n",
        "  #history = model.fit_generator(datagen.flow(X_train, y_train), \n",
        "                                #steps_per_epoch=313,\n",
        "                                #validation_data=(X_test, y_test),\n",
        "                                #epochs=epochs)\n",
        "\n",
        "  history = model.fit_generator(datagen.flow(X_train, y_train), \n",
        "                                steps_per_epoch=313,\n",
        "                                validation_data=valgen.flow(X_test, y_test),\n",
        "                                epochs=epochs, verbose=0)\n",
        "\n",
        "  return history "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OalGTbbYCBqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "1672185b-e605-4561-935d-9e53f11391e4"
      },
      "source": [
        "# Build, compile, train, and test the model\n",
        "\n",
        "model = build_complex_model()\n",
        "\n",
        "# These can be adjusted as needed \n",
        "# Commonly used\n",
        "OPTIMIZER = \"adam\"\n",
        "# This is a multiclass problem so this is appropriate\n",
        "LOSS_FUNC = \"categorical_crossentropy\"\n",
        "METRICS = [\"accuracy\"]\n",
        "# After 30 epochs training loss stops changing\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = compile_model(model, OPTIMIZER, LOSS_FUNC, METRICS)\n",
        "\n",
        "history = train_test_model_with_generator(model, datagen, X_train, y_train,\n",
        "                                          X_test, y_test, EPOCHS, BATCH_SIZE)\n",
        "\n",
        "# Visualize loss function as a function of epoch for model training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Loss vs. Epoch (Model Training)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.show()\n",
        "\n",
        "# Print the final test set accuracy\n",
        "\n",
        "print(\"The training score is \", history.history[\"loss\"][-1], \n",
        "      history.history[\"accuracy\"][-1])\n",
        "\n",
        "print(\"The test score is\", history.history[\"val_loss\"][-1], \n",
        "      history.history[\"val_accuracy\"][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building network.\n",
            "Training network.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The training score is  0.3948061324763989 0.8757045\n",
            "The test score is 0.9454841613769531 0.7745097875595093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6JZukrLt7dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}